#ğŸ§ª ETL Pipeline â€“ Ganesha Maheshwari
This project is an end-to-end ETL pipeline built using Python. It extracts data from five different sources (CSV, Google Sheets, JSON file, Public API, and MongoDB), performs cleaning, transformation, feature engineering, aggregation, and finally uploads the processed data into a PostgreSQL database hosted on Supabase.
ETL_Pipeline_GaneshaMaheshwari_22010221062/
â”œâ”€â”€ etl_pipeline.py              # ETL main script
â”œâ”€â”€ config/
â”‚   â””â”€â”€ credentials.json         # Google Sheets service account credentials
â”‚   â””â”€â”€ db_config.json           # Dummy config for Supabase (optional)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_data.csv          # Example CSV dataset (epidemiology.csv)
â”‚   â”œâ”€â”€ sample_weather.json      # JSON dataset
â”‚   â””â”€â”€ google_sheet_sample.csv  # Exported version of Google Sheets (optional)
â”œâ”€â”€ scheduler.py                 # Automates the ETL using schedule module
â”œâ”€â”€ requirements.txt             # Python package dependencies
â”œâ”€â”€ README.md                    # This documentation file
â”œâ”€â”€ output/
â”‚   â””â”€â”€ final_cleaned_data.csv   # Final cleaned data stored locally
â”œâ”€â”€ load_to_db.py                # Script to upload DataFrame to PostgreSQL (Supabase)
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci_cd.yml            # GitHub Actions CI/CD for automation (optional)
â””â”€â”€ report.pdf                   # PDF report detailing steps, results, and insights


#ğŸš€ ETL Workflow
âœ… 1. Extract
ğŸ“„ CSV: epidemiology.csv

ğŸ“Š Google Sheets: Imported using GSpread and credentials.json

ğŸ§¾ JSON File: data.json

ğŸŒ API: COVID-19 sequences from covid19dataportal.org

ğŸƒ MongoDB: Remote collection from MongoDB Atlas

ğŸ”§ 2. Transform
Column standardization (lowercase, underscore)

Handling missing values using forward-fill

Date formatting to ISO format

Type validation (numeric fields)

Feature engineering:

COVID Impact Score

Days since collection

Sequence length

Aggregation based on date and location_key

ğŸ“¤ 3. Load
Final cleaned dataset is stored as CSV

Additionally, uploaded to a PostgreSQL database on Supabase using SQLAlchemy

#ğŸ› ï¸ Setup Instructions
#ğŸ”— Clone the repository

git clone https://github.com/<your-username>/ETL_Pipeline_GaneshaMaheshwari_22010221062.git
cd ETL_Pipeline_GaneshaMaheshwari_22010221062

git clone https://github.com/<your-username>/ETL_Pipeline_GaneshaMaheshwari.git
cd ETL_Pipeline_GaneshaMaheshwari

#ğŸ“¦ Install dependencies
bash
Copy
Edit
pip install -r requirements.txt
#ğŸ”‘ Setup Google Sheets access
Place your credentials.json in the config/ directory.

Share your Google Sheet with the service account email.

#ğŸ—„ï¸ MongoDB
Make sure your MongoDB Atlas cluster is accessible and the credentials in etl_pipeline.py are valid.

#ğŸ§ª Running the ETL Pipeline
bash
Copy
Edit
python etl_pipeline.py
This will:

Extract data from all sources

Clean and transform the data

Save the result to output/final_cleaned_data.csv

#ğŸ§¬ Uploading to PostgreSQL (Supabase)
To upload your CSV or DataFrame directly to Supabase:

bash
Copy
Edit
python load_to_db.py
Make sure your DB credentials and sqlalchemy connection string are correct in load_to_db.py.

#â° Automate with Scheduler
bash
Copy
Edit
python scheduler.py
This script uses the schedule module to run your ETL at regular intervals (e.g., daily).

ğŸ“‹ Dependencies
pandas

sqlalchemy

psycopg2-binary

gspread

requests

pymongo

schedule

(Full list in requirements.txt)

#ğŸ“„ License
This project is developed for academic purposes and is open for reuse or extension with credit.

âœ¨ Acknowledgements
Supabase

Google Sheets API

MongoDB Atlas

COVID-19 Data Portal
